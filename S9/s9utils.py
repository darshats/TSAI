# -*- coding: utf-8 -*-
"""S8utils.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZASiP6m62FbAEGyXN5EfbxIhUZDbnZeH
"""

from __future__ import print_function
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
import torchvision.transforms.functional as FF
import albumentations as A
from albumentations.augmentations.transforms import Cutout, Normalize, PadIfNeeded, HorizontalFlip
from albumentations.pytorch.transforms import ToTensorV2
from cifardataset import Cifar10SearchDataset
from gradcam import compute_gradcam_image

def create_train_test_sets(mean, std):
    train_transform = A.Compose([Normalize(mean=(mean,mean,mean), std=(std,std,std)),
                                A.Sequential([A.PadIfNeeded(40,40),
                                              A.RandomCrop(32,32)], p=0.5
                                            ),
                                A.HorizontalFlip(),
                                A.Cutout(num_holes=1, max_h_size=8, max_w_size=8),
                                ToTensorV2()
                                ])
    
    trainset = Cifar10SearchDataset(root="~/data/cifar10", 
                                    train=True, 
                                    download=True, 
                                    transform=train_transform
                                   )
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,
                                            shuffle=True, num_workers=2)
    
    
    test_transform = A.Compose([Normalize(mean=(mean, mean, mean), std=(std, std, std)),
                                        ToTensorV2()])
    testset = Cifar10SearchDataset(root='./data', 
                                   train=False,
                                   download=True, 
                                   transform=test_transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=512,
                                          shuffle=False, num_workers=2)

    return trainloader, testloader


def show_internal(imgs):
    if not isinstance(imgs, list):
        imgs = [imgs]
    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)
    for i, img in enumerate(imgs):
        img = img.detach()
        img = FF.to_pil_image(img)
        axs[0, i].imshow(np.asarray(img))
        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])
        
        
def show(incorrect_images, STD, MEAN):
    normalized_images = []
    for i, l in incorrect_images:
        normalized_images.append(i*STD + MEAN)
    normalized_images = np.array(normalized_images)
    t = torch.from_numpy(normalized_images)
    show_internal(torchvision.utils.make_grid(t, 10))
    

def show_with_gradcam(incorrect_images, m, use_cuda, STD, MEAN):
    images = []
    labels = []
    for i, l in incorrect_images:
        images.append(i)
        labels.append(l)
    incorrect_images_tensor = torch.from_numpy(np.array(images))
    label_tensor = torch.from_numpy(np.array(labels))

    ## pass to gradcam. For some reason the final block after GAP doesnt work! (convblock6)
    gradcam_output = compute_gradcam_image(incorrect_images_tensor,
                                           label_tensor,
                                           m, 
                                           [m.convblock5], 
                                           use_cuda, 
                                           STD, 
                                           MEAN)
    gradcam_output = np.array(gradcam_output)
    # gradcam_output = np.swapaxes(gradcam_output, 1, 3)
    t = torch.from_numpy(gradcam_output)
    show_internal(torchvision.utils.make_grid(t, 10))
    
    
## run on GPU find LR max
BATCH = 0

def find_LR_max(m, device, train_loader, optimizer, loss_function, scheduler, x, y):
    global BATCH
    m.train()
    
    torch.autograd.set_detect_anomaly(True)
    for data, target in train_loader:
        BATCH += 1
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = m(data)
        loss = loss_function(output, target)
        
        loss.backward()
        optimizer.step()
        
        ## log loss and lr
        lr = scheduler.get_last_lr()[0]
        l = loss.item()
        x.append(lr)
        y.append(l)
        scheduler.step()

    lr = scheduler.get_last_lr()[0]
    print(f'steps {BATCH}, lr {lr:.5f}')
    return x, y

